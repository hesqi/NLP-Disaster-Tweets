{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib as plt\nimport tensorflow as tf\nimport keras_core as keras\nimport keras_nlp\nfrom tensorflow import keras\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**EXPLORING DATASET**","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-02-13T17:34:30.134394Z","iopub.execute_input":"2024-02-13T17:34:30.135404Z","iopub.status.idle":"2024-02-13T17:34:30.239823Z","shell.execute_reply.started":"2024-02-13T17:34:30.135372Z","shell.execute_reply":"2024-02-13T17:34:30.238808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head(15)","metadata":{"execution":{"iopub.status.busy":"2024-02-13T17:34:30.241123Z","iopub.execute_input":"2024-02-13T17:34:30.24151Z","iopub.status.idle":"2024-02-13T17:34:30.264089Z","shell.execute_reply.started":"2024-02-13T17:34:30.241479Z","shell.execute_reply":"2024-02-13T17:34:30.263077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"test.head(15)","metadata":{}},{"cell_type":"code","source":"test.head(15)","metadata":{"execution":{"iopub.status.busy":"2024-02-13T17:34:30.266373Z","iopub.execute_input":"2024-02-13T17:34:30.26668Z","iopub.status.idle":"2024-02-13T17:34:30.278975Z","shell.execute_reply.started":"2024-02-13T17:34:30.266655Z","shell.execute_reply":"2024-02-13T17:34:30.277987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"length\"] = train[\"text\"].apply(lambda x: len(x))\ntest[\"length\"] = test[\"text\"].apply(lambda x : len(x))\n\nprint(train[[\"text\", \"length\"]].head())\nprint(test[[\"text\", \"length\"]].head())","metadata":{"execution":{"iopub.status.busy":"2024-02-13T17:34:30.280217Z","iopub.execute_input":"2024-02-13T17:34:30.280532Z","iopub.status.idle":"2024-02-13T17:34:30.314387Z","shell.execute_reply.started":"2024-02-13T17:34:30.280489Z","shell.execute_reply":"2024-02-13T17:34:30.313215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting Train Length Stat\nplt.figure(figsize=(10, 6))\nplt.subplot(1, 2, 1)\nplt.hist(train[\"length\"], bins=50, color='blue', edgecolor='black')\nplt.title('Train Data - Text Length Distribution')\nplt.xlabel('Text Length')\nplt.ylabel('Frequency')\n\n# Plotting Test Length Stat\nplt.subplot(1, 2, 2)\nplt.hist(test[\"length\"], bins=50, color='green', edgecolor='black')\nplt.title('Test Data - Text Length Distribution')\nplt.xlabel('Text Length')\nplt.ylabel('Frequency')\n\n# Adjust layout to prevent overlapping\nplt.tight_layout()\n\n# Show plots\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-13T17:34:30.317511Z","iopub.execute_input":"2024-02-13T17:34:30.317792Z","iopub.status.idle":"2024-02-13T17:34:31.003312Z","shell.execute_reply.started":"2024-02-13T17:34:30.317769Z","shell.execute_reply":"2024-02-13T17:34:31.002383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**BERT NLP TOKENIZATION TASK**","metadata":{}},{"cell_type":"code","source":"batch_size = 64\nnum_training_examples = train.shape[0]\ntrain_split = 0.8\nval_split = 0.2\nsteps_per_epoch = int(num_training_examples) * train_split // batch_size\n\nepochs = 2\nauto = tf.data.experimental.AUTOTUNE","metadata":{"execution":{"iopub.status.busy":"2024-02-13T17:34:31.004627Z","iopub.execute_input":"2024-02-13T17:34:31.004903Z","iopub.status.idle":"2024-02-13T17:34:31.009769Z","shell.execute_reply.started":"2024-02-13T17:34:31.00488Z","shell.execute_reply":"2024-02-13T17:34:31.00885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train[\"text\"]\ny = train[\"target\"]\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=val_split, random_state=42)\n\nX_test = test[\"text\"]","metadata":{"execution":{"iopub.status.busy":"2024-02-13T17:34:31.011046Z","iopub.execute_input":"2024-02-13T17:34:31.011345Z","iopub.status.idle":"2024-02-13T17:34:31.023203Z","shell.execute_reply.started":"2024-02-13T17:34:31.011321Z","shell.execute_reply":"2024-02-13T17:34:31.022354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom tqdm import tqdm\nfrom transformers import BertForSequenceClassification, BertTokenizer, get_linear_schedule_with_warmup\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-02-13T17:34:31.024132Z","iopub.execute_input":"2024-02-13T17:34:31.024441Z","iopub.status.idle":"2024-02-13T17:34:36.861149Z","shell.execute_reply.started":"2024-02-13T17:34:31.024407Z","shell.execute_reply":"2024-02-13T17:34:36.860235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\nmodel = BertForSequenceClassification.from_pretrained('bert-large-uncased', num_labels=2)\n\n# Tokenize and encode the training data\ntrain_inputs = tokenizer(list(X_train), padding=True, truncation=True, return_tensors='pt')\ntrain_labels = torch.tensor(list(y_train))","metadata":{"execution":{"iopub.status.busy":"2024-02-13T17:34:36.863819Z","iopub.execute_input":"2024-02-13T17:34:36.864436Z","iopub.status.idle":"2024-02-13T17:34:50.846545Z","shell.execute_reply.started":"2024-02-13T17:34:36.864407Z","shell.execute_reply":"2024-02-13T17:34:50.845262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_inputs = tokenizer(list(X_val), padding=True, truncation=True, return_tensors='pt')\nval_labels = torch.tensor(list(y_val))\n\n# Tokenize and encode the test data\ntest_inputs = tokenizer(list(test['text']), padding=True, truncation=True, return_tensors='pt')\n\n# Create DataLoader for efficient processing\ntrain_dataset = TensorDataset(train_inputs['input_ids'], train_inputs['attention_mask'], train_labels)\nval_dataset = TensorDataset(val_inputs['input_ids'], val_inputs['attention_mask'], val_labels)\ntest_dataset = TensorDataset(test_inputs['input_ids'], test_inputs['attention_mask'])\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)\ntest_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-02-13T17:34:50.848356Z","iopub.execute_input":"2024-02-13T17:34:50.848866Z","iopub.status.idle":"2024-02-13T17:34:55.408126Z","shell.execute_reply.started":"2024-02-13T17:34:50.848826Z","shell.execute_reply":"2024-02-13T17:34:55.407131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-02-13T17:34:55.409291Z","iopub.execute_input":"2024-02-13T17:34:55.40957Z","iopub.status.idle":"2024-02-13T17:34:55.991401Z","shell.execute_reply.started":"2024-02-13T17:34:55.409546Z","shell.execute_reply":"2024-02-13T17:34:55.990406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\ncriterion = torch.nn.CrossEntropyLoss()\n# Gradient Clipping\nepochs = 3\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader) * epochs)\n\nmax_grad_norm = 1.0\n\n# Training loop\nepochs = 3\nfor epoch in range(epochs):\n    model.train()\n    for batch in tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{epochs}'):\n        batch = tuple(t.to(device) for t in batch)\n        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n        optimizer.zero_grad()\n        outputs = model(**inputs)\n        loss = criterion(outputs.logits, inputs['labels'])\n        loss.backward()\n        # Gradient Clipping\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n        optimizer.step()\n        scheduler.step()","metadata":{"execution":{"iopub.status.busy":"2024-02-13T17:34:55.99272Z","iopub.execute_input":"2024-02-13T17:34:55.993011Z","iopub.status.idle":"2024-02-13T17:54:03.968897Z","shell.execute_reply.started":"2024-02-13T17:34:55.992987Z","shell.execute_reply":"2024-02-13T17:54:03.967983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\nval_preds = []\nwith torch.no_grad():\n    for batch in tqdm(val_dataloader, desc='Validation'):\n        batch = tuple(t.to(device) for t in batch)\n        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n        outputs = model(**inputs)\n        val_preds.extend(torch.argmax(outputs.logits, axis=1).cpu().numpy())\n\n# Evaluate the model\nprint(\"Classification Report:\")\nprint(classification_report(y_val, val_preds))\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_val, val_preds))\nprint(\"F1 Score:\", f1_score(y_val, val_preds))","metadata":{"execution":{"iopub.status.busy":"2024-02-13T17:54:03.970164Z","iopub.execute_input":"2024-02-13T17:54:03.970488Z","iopub.status.idle":"2024-02-13T17:54:25.98417Z","shell.execute_reply.started":"2024-02-13T17:54:03.970461Z","shell.execute_reply":"2024-02-13T17:54:25.983237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The F1 score in the provided output is 0.8188, indicating a good balance between precision and recall in the classification model.","metadata":{}},{"cell_type":"code","source":"model.eval()\ntest_preds = []\nwith torch.no_grad():\n    for batch in tqdm(test_dataloader, desc='Testing'):\n        batch = tuple(t.to(device) for t in batch)\n        inputs = {'input_ids': batch[0], 'attention_mask': batch[1]}\n        outputs = model(**inputs)\n        test_preds.extend(torch.argmax(outputs.logits, axis=1).cpu().numpy())\n\n# Add predictions to the test_data DataFrame\ntest['target'] = test_preds\n\n# Save the predictions to a CSV file\ntest[['id', 'target']].to_csv('predictions_bert.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-02-13T17:56:00.133123Z","iopub.execute_input":"2024-02-13T17:56:00.133508Z","iopub.status.idle":"2024-02-13T17:56:44.53885Z","shell.execute_reply.started":"2024-02-13T17:56:00.133478Z","shell.execute_reply":"2024-02-13T17:56:44.537911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test[['id', 'target']].head(5))\nprint(test['target'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2024-02-13T18:10:33.986291Z","iopub.execute_input":"2024-02-13T18:10:33.986676Z","iopub.status.idle":"2024-02-13T18:10:34.003117Z","shell.execute_reply.started":"2024-02-13T18:10:33.986649Z","shell.execute_reply":"2024-02-13T18:10:34.002083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\n# Assuming you have already calculated the confusion matrix\nconf_matrix = confusion_matrix(y_val, val_preds)\n\n# Plot the confusion matrix\nplt.figure(figsize=(8, 6))\nsns.set(font_scale=1.2)  # Adjust font scale for better visualization\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap=\"Blues\", cbar=False,\n            xticklabels=[\"Not Disaster\",\"Disaster\"],\n            yticklabels=[\"Not Disaster\",\"Disaster\"])\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix on Validation dataset')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-13T18:05:53.270437Z","iopub.execute_input":"2024-02-13T18:05:53.271059Z","iopub.status.idle":"2024-02-13T18:05:53.43253Z","shell.execute_reply.started":"2024-02-13T18:05:53.271027Z","shell.execute_reply":"2024-02-13T18:05:53.431683Z"},"trusted":true},"execution_count":null,"outputs":[]}]}